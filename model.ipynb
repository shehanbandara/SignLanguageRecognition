{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10230fc-7372-4d83-b4da-1018d8924194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb3c07-0282-454e-b4ef-9e3a9e844522",
   "metadata": {},
   "source": [
    "# Load The Sign Language Gesture Images Dataset\n",
    "\n",
    "Download: https://www.kaggle.com/ahmedkhanak1995/sign-language-gesture-images-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eea98b9-971c-4b8d-8726-97acdf12d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "# Sort the data folders in alphabetical order\n",
    "dataFolders = sorted(os.listdir('SignLanguageRecognitionData/'))\n",
    "\n",
    "# Delete .DS_Store if present\n",
    "if dataFolders[0] == '.DS_Store':\n",
    "    dataFolders.pop(0)\n",
    "    \n",
    "print(dataFolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5e9772-f3fc-42b6-855e-f289a10f2f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 50, 50, 3)\n",
      "(36000,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for all images and labels in the dataset\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Loop through all data folders\n",
    "for i, label in enumerate(dataFolders):\n",
    "    \n",
    "    # Loop through all images in the data folder\n",
    "    for j in os.listdir('SignLanguageRecognitionData/' + label):\n",
    "        \n",
    "        # Resize the image\n",
    "        image = cv2.resize(cv2.imread('SignLanguageRecognitionData/' + label + '/' + j), (50,50))\n",
    "        \n",
    "        # Append the image and label to its respective list\n",
    "        images.append(image)\n",
    "        labels.append(i)\n",
    "\n",
    "# Convert the images and labels lists to arrays\n",
    "images = np.asarray(images)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "# Print the shape of the images and labels arrays\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c995af-ac43-47b5-ab8c-4821c5392df9",
   "metadata": {},
   "source": [
    "# Split The Dataset Into Training, Validation, & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30834e1-3c82-437b-a377-a36281792fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 80% training data, 10% validation data, & 10% testing data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.2)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41656569-512e-4acc-82da-0cc1136791f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28800, 50, 50, 3) (28800,)\n",
      "(3600, 50, 50, 3) (3600,)\n",
      "(3600, 50, 50, 3) (3600,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the training, validation, and testing data\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d362a65d-f60a-4ef5-a551-e6c274216540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Y_train, Y_val, & Y_test to One-Hot Encoded Vectors\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_val = to_categorical(Y_val)\n",
    "Y_test = to_categorical(Y_test)\n",
    "X_train = X_train / 255.\n",
    "X_val = X_val / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc950f-e2e7-4478-bec4-8f1feabce606",
   "metadata": {},
   "source": [
    "# Build The Sign Language Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa80fab-7a17-496d-a175-74bcd2ff3bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 16)        448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 46, 46, 16)        2320      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 44, 44, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 22, 22, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 20, 20, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 18, 18, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                3096      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,568\n",
      "Trainable params: 156,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the Sign Language Recognition model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation ='relu', input_shape=(50,50,3)),\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation ='relu'),\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation ='relu'),\n",
    "    tf.keras.layers.MaxPool2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n",
    "    tf.keras.layers.MaxPool2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(24, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d97bef-0127-4036-8aa1-a54c4925cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7371c-dee3-41c8-8343-ba866670441b",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc98db71-b3e3-4bcf-a11a-0aebd22d0e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "900/900 [==============================] - 128s 141ms/step - loss: 0.3730 - accuracy: 0.8841 - val_loss: 0.0266 - val_accuracy: 0.9917\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 131s 146ms/step - loss: 0.0245 - accuracy: 0.9928 - val_loss: 0.0112 - val_accuracy: 0.9958\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 136s 151ms/step - loss: 0.0392 - accuracy: 0.9901 - val_loss: 0.0070 - val_accuracy: 0.9981\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 140s 156ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 0.0048 - val_accuracy: 0.9989\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - 132s 147ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 9.7028e-06 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 127s 141ms/step - loss: 6.6835e-06 - accuracy: 1.0000 - val_loss: 3.4995e-06 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - 131s 146ms/step - loss: 2.7319e-06 - accuracy: 1.0000 - val_loss: 1.6793e-06 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "900/900 [==============================] - 139s 154ms/step - loss: 1.3652e-06 - accuracy: 1.0000 - val_loss: 8.8444e-07 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "900/900 [==============================] - 125s 139ms/step - loss: 7.4532e-07 - accuracy: 1.0000 - val_loss: 4.9808e-07 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "900/900 [==============================] - 122s 135ms/step - loss: 4.2357e-07 - accuracy: 1.0000 - val_loss: 2.8749e-07 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8bdaa7ad60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs=10, verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a9a0f-aee6-4148-a7e4-97329d421ee9",
   "metadata": {},
   "source": [
    "# Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c57ea2-1b87-4e22-a198-9efe3f6a6077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 1s 11ms/step - loss: 3.6237e-07 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.623691497978143e-07, 1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0747781-7522-4bf0-a38e-e327f93b246b",
   "metadata": {},
   "source": [
    "# Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825348fc-2dd6-4f3c-8d7d-38b6e93e92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83865304-fab4-4c17-909c-1a35649f18d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
